{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "860abdb1",
   "metadata": {},
   "source": [
    "# Rock-Paper-Scissors Image Classification with PyTorch\n",
    "\n",
    "In this notebook, we will build a Convolutional Neural Network (CNN) using PyTorch to classify images of rock, paper, and scissors. We will train the model on a dataset of images and evaluate its performance on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e98aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc30cbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cefb6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard writer\n",
    "writer = SummaryWriter('runs/tree_recognition_experiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac60aa5",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Dataset\n",
    "\n",
    "We will use `ImageFolder` to load our dataset, which expects images to be organized in subdirectories based on their class labels. We will apply transformations to resize the images and convert them to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e6621a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['birch', 'maple', 'pine', 'rowan', 'spruce']\n"
     ]
    }
   ],
   "source": [
    "# Transformations\n",
    "transform = Compose([Resize((28, 28)), ToTensor()])\n",
    "\n",
    "# Load the dataset\n",
    "train_data = ImageFolder(root='train_data', transform=transform)\n",
    "val_data = ImageFolder(root='val_data', transform=transform)\n",
    "\n",
    "# Print class names\n",
    "print('Classes:', train_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0def71a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500e7b3b",
   "metadata": {},
   "source": [
    "#### Display some samples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths to the images\n",
    "image_paths = [\n",
    "    \"rps/paper/paper02-089.png\",\n",
    "    \"rps/rock/rock06ck02-100.png\",\n",
    "    \"rps/scissors/testscissors02-006.png\"\n",
    "]\n",
    "\n",
    "# Load the images\n",
    "images = [Image.open(image_path) for image_path in image_paths]\n",
    "titles = ['Paper', 'Rock', 'Scissors']\n",
    "\n",
    "# Display the images\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 5))\n",
    "for ax, image, title in zip(axs, images, titles):\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')  # Hide axis ticks\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ba57f",
   "metadata": {},
   "source": [
    "## Define a Simple CNN Model\n",
    "\n",
    "We will define a simple Convolutional Neural Network (CNN) with two convolutional layers followed by two fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74bb6877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, len(train_data.classes))  # Number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 7 * 7)  # Flattening the tensor for the fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0dbdc",
   "metadata": {},
   "source": [
    "## Initialize the Model, Loss Function, and Optimizer\n",
    "\n",
    "We will create an instance of the CNN model, define the loss function as Cross Entropy Loss, and use SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42181f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = SimpleCNN().to(device)  # Move the model to the appropriate device\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0879f14b",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "We will define a function to train the model and monitor its performance on the validation set after each epoch. The best model weights will be saved based on validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71164087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_train_loss = 0.0\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        running_val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Log losses and accuracy to TensorBoard\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/validation', val_accuracy, epoch)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_weights.pth')\n",
    "            print(f\"Model saved at epoch {epoch+1}\")\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50864f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.4963, Val Loss: 1.9551, Val Accuracy: 24.39%\n",
      "Model saved at epoch 1\n",
      "Epoch [2/10], Train Loss: 1.4882, Val Loss: 1.5479, Val Accuracy: 24.39%\n",
      "Model saved at epoch 2\n",
      "Epoch [3/10], Train Loss: 1.3927, Val Loss: 1.6174, Val Accuracy: 24.39%\n",
      "Epoch [4/10], Train Loss: 1.2537, Val Loss: 1.5275, Val Accuracy: 31.71%\n",
      "Model saved at epoch 4\n",
      "Epoch [5/10], Train Loss: 1.0986, Val Loss: 1.6746, Val Accuracy: 29.27%\n",
      "Epoch [6/10], Train Loss: 1.1976, Val Loss: 1.2727, Val Accuracy: 48.78%\n",
      "Model saved at epoch 6\n",
      "Epoch [7/10], Train Loss: 0.8468, Val Loss: 0.9692, Val Accuracy: 63.41%\n",
      "Model saved at epoch 7\n",
      "Epoch [8/10], Train Loss: 0.7193, Val Loss: 0.8144, Val Accuracy: 70.73%\n",
      "Model saved at epoch 8\n",
      "Epoch [9/10], Train Loss: 0.5858, Val Loss: 0.7422, Val Accuracy: 75.61%\n",
      "Model saved at epoch 9\n",
      "Epoch [10/10], Train Loss: 0.4881, Val Loss: 0.8509, Val Accuracy: 65.85%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a201f5f",
   "metadata": {},
   "source": [
    "## Load the Best Model for Inference\n",
    "\n",
    "After training, we will load the best model weights saved during training for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "354192d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jamiv\\AppData\\Local\\Temp\\ipykernel_16616\\2409945200.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model_weights.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=1568, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best model for inference\n",
    "model.load_state_dict(torch.load('best_model_weights.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec813de3",
   "metadata": {},
   "source": [
    "## Perform Inference on a Single Image\n",
    "\n",
    "We will define a function to perform inference on a single image and predict its class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cff214f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform inference on a single image\n",
    "from PIL import Image\n",
    "\n",
    "def infer_single_image(model, image_path, transform, classes):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Move to device\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predicted_class = classes[predicted.item()]\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca42b897",
   "metadata": {},
   "source": [
    "## Just to Make It More Interactive, I Added Gradio Flavor!\n",
    "\n",
    "*Warning:* This notebook now contains traces of Gradio. Side effects may include uncontrollable excitement and a sudden urge to classify everything you see!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87a81407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (4.44.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: fastapi<1.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (0.115.0)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (1.3.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (6.4.5)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (3.8.4)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (3.10.7)\n",
      "Requirement already satisfied: packaging in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (10.3.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (2.9.2)\n",
      "Requirement already satisfied: pydub in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (0.0.12)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (0.6.8)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (4.11.0)\n",
      "Requirement already satisfied: urllib3~=2.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio) (0.31.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio-client==1.3.0->gradio) (2024.3.1)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from fastapi<1.0->gradio) (0.38.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from httpx>=0.24.1->gradio) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
      "Requirement already satisfied: requests in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\jamiv\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.3.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\jamiv\\appdata\\roaming\\python\\python312\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jamiv\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jamiv\\.conda\\envs\\solutionspr\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://c02beb79d04f0f0faa.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c02beb79d04f0f0faa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install Gradio if not already installed\n",
    "!pip install gradio\n",
    "\n",
    "# Import Gradio\n",
    "import gradio as gr\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(image):\n",
    "    # Apply the same transformations as during training\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predicted_class = train_data.classes[predicted.item()]\n",
    "    return predicted_class\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Tree recognition\",\n",
    "    description=\"Upload an image of birch, maple, pine, rowan or spruce, and the model will predict its class.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SolutionsPR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
