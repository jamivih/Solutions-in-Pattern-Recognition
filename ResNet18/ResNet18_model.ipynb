{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import wandb\n",
    "import gradio as gr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Weights & Biases (W&B) for Experiment Tracking\n",
    "\n",
    "In this block, we initialize **Weights & Biases (W&B)** to track and log our experiment. W&B is a popular tool used in machine learning for tracking model performance, visualizing training progress, and organizing results.\n",
    "\n",
    "- **`wandb.init()`**: This initializes the W&B project and sets the configuration for the experiment.\n",
    "    - **`project`**: The name of the project on W&B. In this case, it’s called `\"resnet18-transfer-learning-hamk\"`, indicating that we will be using the ResNet18 model for transfer learning.\n",
    "    - **`config`**: A dictionary that defines key parameters of our experiment:\n",
    "        - **`epochs`**: The number of training epochs (in this case, 10).\n",
    "        - **`batch_size`**: The size of each batch during training (set to 16).\n",
    "        - **`learning_rate`**: The initial learning rate for the optimizer (set to 0.001).\n",
    "        - **`architecture`**: The pre-trained model architecture we will use (ResNet18).\n",
    "        - **`dataset`**: The name of the dataset (Rock Paper Scissors, abbreviated as \"RPS\").\n",
    "\n",
    "By using W&B, we can easily monitor our model’s performance, visualize metrics such as loss and accuracy, and compare different runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B and set up the project\n",
    "wandb.init(project=\"\", config={\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"architecture\": \"ResNet18\",\n",
    "    \"dataset\": \"tree_detection\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Dataset and Data Loaders\n",
    "\n",
    "In this block, we define the necessary transformations for our dataset, load the data, and create data loaders for training and validation.\n",
    "\n",
    "### Transformations\n",
    "- **`Compose()`**: We use `Compose` to apply a series of transformations to the images.\n",
    "    - **`Resize((28, 28))`**: This resizes the images to a fixed size of 28x28 pixels, which is suitable for the model input.\n",
    "    - **`ToTensor()`**: This converts the images into tensors, which are required by PyTorch for model training.\n",
    "\n",
    "### Loading the Dataset\n",
    "- **`ImageFolder()`**: This utility is used to load images stored in folder structures. Each folder corresponds to a class label, and the images are automatically labeled based on the folder names.\n",
    "    - **`train_data`**: The training dataset is loaded from the `\"rps\"` directory, which contains images for Rock Paper Scissors.\n",
    "    - **`val_data`**: The validation dataset is loaded from the `\"rps-test-set\"` directory.\n",
    "    - **`train_data.classes`**: This prints out the class labels (Rock, Paper, Scissors) from the dataset.\n",
    "\n",
    "### Data Loaders\n",
    "- **`DataLoader()`**: This utility loads data in batches and shuffles it for training or validation.\n",
    "    - **`train_loader`**: Loads the training dataset with a batch size of 16 and shuffles the data.\n",
    "    - **`val_loader`**: Loads the validation dataset with a batch size of 16 but without shuffling.\n",
    "\n",
    "The final lines of the block print the number of samples in both the training and validation sets. These data loaders will be used to feed data into the model during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations (from your lab)\n",
    "transform = Compose([Resize((224, 224)), ToTensor()])\n",
    "\n",
    "# Load the dataset\n",
    "train_data = ImageFolder(root='train_data', transform=transform)\n",
    "val_data = ImageFolder(root='val_data', transform=transform)\n",
    "print('Classes:', train_data.classes)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=False)\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Pre-trained ResNet Model\n",
    "\n",
    "In this block, we load the pre-trained **ResNet18** model, which is a popular deep learning architecture known for its residual connections that help avoid the vanishing gradient problem in deep networks.\n",
    "\n",
    "- **`models.resnet18()`**: This function loads the **ResNet18** architecture from the **torchvision** library.\n",
    "    - **`weights=models.ResNet18_Weights.DEFAULT`**: This specifies that we are using the pre-trained weights provided by **torchvision**. These weights are trained on a large-scale dataset like ImageNet, allowing us to benefit from a model that has already learned useful features.\n",
    "- **`.to(device)`**: This moves the model to the specified device (usually a GPU or CPU). If a GPU is available, the model will run on it for faster computations.\n",
    "\n",
    "By using this pre-trained model, we leverage the knowledge it has already learned and adapt it to our specific task (transfer learning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained ResNet model\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the Model Summary\n",
    "\n",
    "In this block, we use the **torchsummary** library to display a detailed summary of the **ResNet18** model architecture. This helps us understand the structure of the model and the number of parameters involved.\n",
    "\n",
    "- **`summary(model, input_size=(3, 224, 224))`**: This function provides a layer-by-layer summary of the model architecture.\n",
    "    - **Input size**: For ResNet18, the expected input size is a 3-channel image (RGB) with dimensions 224x224 pixels.\n",
    "\n",
    "### Model Summary\n",
    "- **Layer (type)**: Lists each layer in the model, such as convolutional layers, batch normalization, ReLU activations, and fully connected layers.\n",
    "- **Output Shape**: Displays the shape of the output after each layer.\n",
    "- **Param #**: Shows the number of parameters in each layer, including trainable parameters like weights and biases.\n",
    "\n",
    "The model has approximately **11.7 million parameters**, all of which are trainable. This is a relatively small number compared to deeper models like ResNet50, making ResNet18 a good choice for transfer learning tasks with limited computational resources.\n",
    "\n",
    "The summary provides the estimated memory usage for the input size, forward/backward passes, and the parameters, giving us a clear idea of the model's complexity and memory requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model summary (for an input size of (3, 224, 224) for ResNet)\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the Final Fully Connected Layer for Transfer Learning\n",
    "\n",
    "In transfer learning, we often modify the final layer of a pre-trained model to adapt it to our specific task. In this case, we are fine-tuning the **ResNet18** model for the tree dataset, which has 5 output classes.\n",
    "\n",
    "- **`model.fc = nn.Linear(512, 5)`**: \n",
    "    - The final fully connected (FC) layer of ResNet18 is originally designed for 1000 classes (as it was trained on ImageNet).\n",
    "    - We replace the FC layer with a new `Linear` layer that has 512 input features (from the last layer of ResNet18) and 5 output features.\n",
    "    - **`.to(device)`**: Moves the modified layer to the device (GPU or CPU).\n",
    "\n",
    "By modifying this layer, we retain all the features learned by the pre-trained model and fine-tune the final layer to classify images into the 3 target categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the final fully connected layer for transfer learning (assuming 3 classes for the Rock-Paper-Scissors dataset)\n",
    "model.fc = nn.Linear(512, 5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Training and Validation Loops\n",
    "\n",
    "In this section, we define two key functions: one for training the model and one for validating its performance. These loops are fundamental to the transfer learning process, where the pre-trained model is fine-tuned on our specific dataset.\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "The **`train()`** function handles the training process for one epoch, performing the following steps:\n",
    "\n",
    "- **`model.train()`**: Sets the model to training mode, enabling features like dropout and batch normalization.\n",
    "- **Forward pass**: The input images are passed through the model, and the output predictions are generated.\n",
    "- **Loss calculation**: The loss between the model's predictions and the true labels is computed using the specified criterion (e.g., cross-entropy loss).\n",
    "- **Backward pass**: The gradients of the loss with respect to the model parameters are computed (via `loss.backward()`), and the optimizer updates the model weights.\n",
    "- **Accuracy calculation**: The number of correctly predicted labels is compared against the total number of labels to calculate the training accuracy.\n",
    "- **W&B logging**: The loss and accuracy for the current epoch are logged to **Weights & Biases** (W&B), allowing us to track the model's performance.\n",
    "\n",
    "### Validation Loop\n",
    "\n",
    "The **`validate()`** function handles the evaluation of the model on the validation set:\n",
    "\n",
    "- **`model.eval()`**: Sets the model to evaluation mode, disabling certain features like dropout.\n",
    "- **No gradient calculation**: The validation loop is wrapped in `torch.no_grad()` to prevent gradient calculations and save memory during the evaluation.\n",
    "- **Forward pass**: Similar to the training loop, the input images are passed through the model, but no backpropagation or optimization is performed.\n",
    "- **Accuracy and loss calculation**: The loss and accuracy are computed for the validation set.\n",
    "- **W&B logging**: The validation loss and accuracy are logged to W&B to monitor the model's performance on unseen data.\n",
    "\n",
    "Both loops track key metrics, including loss and accuracy, and use **Weights & Biases** to log these metrics for real-time monitoring of the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_accuracy = 100. * correct / total\n",
    "\n",
    "    # Log the metrics to W&B\n",
    "    wandb.log({\"Train Loss\": epoch_loss, \"Train Accuracy\": epoch_accuracy})\n",
    "    print(f\"Train Loss: {epoch_loss}, Train Accuracy: {epoch_accuracy}%\")\n",
    "    return epoch_loss\n",
    "\n",
    "# Validation loop\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_accuracy = 100. * correct / total\n",
    "    # Log the metrics to W&B\n",
    "    wandb.log({\"Validation Loss\": epoch_loss, \"Validation Accuracy\": epoch_accuracy})\n",
    "\n",
    "    print(f\"Validation Loss: {epoch_loss}, Validation Accuracy: {epoch_accuracy}%\")\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "In this block, we train the model for a set number of epochs using the previously defined **`train()`** and **`validate()`** functions. The model will learn from the training data and be evaluated on the validation set at the end of each epoch.\n",
    "\n",
    "- **`num_epochs = 10`**: Specifies that the model will be trained for 10 epochs. This can be adjusted based on the dataset size, model complexity, and available resources.\n",
    "- **Training Loop**: \n",
    "    - For each epoch, the **`train()`** function is called to train the model on the training set.\n",
    "    - After each epoch, the **`validate()`** function evaluates the model on the validation set to track its performance.\n",
    "    - For each epoch, the loss and accuracy for both the training and validation sets are printed to monitor progress.\n",
    "\n",
    "### Weights & Biases (W&B) Logging\n",
    "- **`wandb.watch(model)`**: This function allows **W&B** to track the model's gradients and parameters throughout training, enabling more detailed insights and visualizations into the model’s training process.\n",
    "\n",
    "By logging each epoch's results and using **W&B** to track the model’s progress, we ensure a well-documented and transparent training process. After training, the model can be saved and used for future inference or further fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "best_val_loss = float(('inf'))\n",
    "num_epochs = 20\n",
    "train_loss_data = []\n",
    "val_loss_data = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    train_loss_data.append(train_loss)\n",
    "    val_loss_data.append(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        # Save the model to W&B\n",
    "        wandb.watch(model)\n",
    "        # If you want to save the model to disk \n",
    "        torch.save(model.state_dict(), 'resnet18_transfer_learning.pth')\n",
    "        print(f'Best model saved at epoch {epoch+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the best saved model\n",
    "model.load_state_dict(torch.load('resnet18_transfer_learning.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting training and validation loss for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(num_epochs), train_loss_data, label='Training Loss', marker='o')\n",
    "plt.plot(range(num_epochs), val_loss_data, label='Validation Loss', marker='s')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform inference on a single image\n",
    "def infer(model, image_path, transform, device):\n",
    "    # Load the image and convert it to RGB (3 channels)\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = output.max(1)\n",
    "    \n",
    "    return predicted.item()\n",
    "\n",
    "# Class names are based on the order printed from train_data.classes\n",
    "class_names = ['birch', 'maple', 'pine', 'rowan', 'spruce']\n",
    "\n",
    "# Path to the test image\n",
    "image_path = 'test_data\\pine\\IMG-20241007-WA0017_jpg.rf.a2d4123d01a1dac6d5981c347ec459c9.jpg'  # Adjust the path as needed\n",
    "\n",
    "# Run inference\n",
    "predicted_class = infer(model, image_path, transform, device)\n",
    "print(f'Predicted class: {class_names[predicted_class]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model with a Confusion Matrix\n",
    "\n",
    "In this block, we generate predictions for the entire validation set and display a **confusion matrix** to better understand how well the model is performing on each tree class.\n",
    "\n",
    "### Getting Predictions\n",
    "\n",
    "The **`get_all_predictions()`** function:\n",
    "- **`model.eval()`**: Sets the model to evaluation mode to disable training-specific features such as dropout.\n",
    "- **Predictions**: For each batch in the validation loader, we pass the inputs through the model and record the predicted class labels.\n",
    "    - The predictions are stored in **`all_preds`**, and the true labels in **`all_labels`**.\n",
    "    - Both predictions and labels are moved back to the CPU using `.cpu()` and then converted to numpy arrays for further processing.\n",
    "- **Concatenation**: After looping through the entire validation set, we concatenate the predictions and labels into single arrays for easy comparison.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get predictions for the entire validation set\n",
    "def get_all_predictions(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    # Convert list of arrays to single numpy arrays\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Get predictions and true labels from validation data\n",
    "preds, labels = get_all_predictions(model, val_loader, device)\n",
    "# Print the shape of preds and labels\n",
    "print(f\"Number of predictions: {len(preds)}\")\n",
    "print(f\"Number of true labels: {len(labels)}\")\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(labels, preds)\n",
    "\n",
    "# Display the confusion matrix\n",
    "class_names = ['birch', 'maple', 'pine', 'rowan', 'spruce']\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model with test dataset\n",
    "The test dataset consists of self-taken pictures of each tree species\n",
    "- Accuracy, precision, recall, and F1-score\n",
    "- Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_data = ImageFolder(root='test_data', transform=transform)  # Adjust the path as needed\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "# Load the best model\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Initialize lists to hold the true labels and predictions\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "# Iterate over the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)  # Move inputs to the appropriate device\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)  # Get the predicted class\n",
    "\n",
    "        # Store true labels and predicted labels\n",
    "        true_labels.extend(labels.cpu().numpy())  # Move labels to CPU and convert to numpy\n",
    "        predictions.extend(predicted.cpu().numpy())  # Move predictions to CPU and convert to numpy\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions, average='weighted')  # Use 'macro' or 'micro' if preferred\n",
    "recall = recall_score(true_labels, predictions, average='weighted')  # Use 'macro' or 'micro' if preferred\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')  # Use 'macro' or 'micro' if preferred\n",
    "\n",
    "# Print results\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Class names (use the same order as in ImageFolder)\n",
    "class_names = test_data.classes\n",
    "\n",
    "# Plot the confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion Matrix for Test Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on a Single Image and Logging Results to W&B\n",
    "\n",
    "In this block, we define a function to perform inference on a single image using the pre-trained model and log the results to **Weights & Biases (W&B)**. This is useful for evaluating the model's performance on individual images and visualizing the results.\n",
    "\n",
    "### Function: `infer_and_log()`\n",
    "\n",
    "- **Loading and Preprocessing**: \n",
    "    - **`Image.open(image_path).convert('RGB')`**: Opens the image from the given path and converts it to an RGB format (3-channel).\n",
    "    - **`transform(image)`**: Applies the transformation (e.g., resizing, tensor conversion) to the image, similar to how we transformed the training and validation images.\n",
    "    - **`.unsqueeze(0)`**: Adds a batch dimension to the image tensor, as the model expects input in batches.\n",
    "    - The image tensor is then moved to the specified device (GPU or CPU) using **`.to(device)`**.\n",
    "\n",
    "- **Inference**:\n",
    "    - **`model.eval()`**: The model is set to evaluation mode to disable any training-specific operations.\n",
    "    - **`torch.no_grad()`**: Disables gradient calculations to save memory and speed up inference.\n",
    "    - The model processes the image, and the predicted class is obtained by finding the index of the highest output score using **`.max(1)`**.\n",
    "    - The predicted class is then mapped to its corresponding label using the **`class_names`** list.\n",
    "\n",
    "- **Logging to Weights & Biases**:\n",
    "    - **`wandb.log()`**: Logs the original input image along with the predicted class to **W&B** for easy tracking and visualization.\n",
    "    - **`wandb.Image()`**: This function is used to log the input image along with a caption showing the predicted class.\n",
    "\n",
    "### Example Usage\n",
    "- **Inference and Logging**: The `infer_and_log()` function is called, which performs inference on the test image and logs the results to **W&B**.\n",
    "\n",
    "The predicted class is then printed to the console to verify the result.\n",
    "\n",
    "By using this function, you can easily evaluate the model's predictions on individual images and track those predictions through **Weights & Biases** for better analysis and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform inference on a single image and log results to W&B\n",
    "def infer_and_log(model, image_path, transform, device, class_names):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        _, predicted = outputs.max(1)\n",
    "        predicted_class = class_names[predicted.item()]\n",
    "    \n",
    "    # Log the input image and the prediction to W&B\n",
    "    wandb.log({\n",
    "        \"Image\": wandb.Image(image, caption=f\"Predicted: {predicted_class}\"),\n",
    "        \"Prediction\": predicted_class\n",
    "    })\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "class_names = ['birch', 'maple', 'pine', 'rowan', 'spruce']\n",
    "\n",
    "# Path to the test image\n",
    "image_path = 'test_data\\pine\\IMG-20241007-WA0017_jpg.rf.a2d4123d01a1dac6d5981c347ec459c9.jpg'\n",
    "\n",
    "# Run inference and log results to W&B\n",
    "predicted_class = infer_and_log(model, image_path, transform, device, class_names)\n",
    "print(f'Predicted class: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform inference on a single image\n",
    "def infer_gradio(image):\n",
    "    # Apply the same transform as used in training\n",
    "    image = image.convert('RGB')  # Ensure it's in 3-channel format\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    \n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = output.max(1)\n",
    "    \n",
    "    return class_names[predicted.item()]  # Return the predicted class name\n",
    "\n",
    "# Class names (modify according to your tree dataset classes)\n",
    "class_names = ['birch', 'maple', 'pine', 'rowan', 'spruce']  # Example classes\n",
    "\n",
    "# Gradio Interface\n",
    "interface = gr.Interface(\n",
    "    fn=infer_gradio,  # The function for inference\n",
    "    inputs=gr.Image(type=\"pil\"),  # Input: an image file\n",
    "    outputs=\"text\",  # Output: the predicted class name\n",
    "    title=\"Tree Species Detection\",  # Title of the app\n",
    "    description=\"Upload an image of a tree, and the model will predict its species.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SolutionsPR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
